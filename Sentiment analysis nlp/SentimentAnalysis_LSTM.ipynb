{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM,  Dropout\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3 yıldır tık demedi. :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3 yıldır kullanıyorum müthiş</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Ürün bugün elime geçti çok fazla inceleme fırs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Almaya karar verdim. Hemencecik geldi. Keyifle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Günlük kullanımınızı çok çok iyi karsılıyor kı...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                             Review\n",
       "0       1                            3 yıldır tık demedi. :)\n",
       "1       1                      3 yıldır kullanıyorum müthiş \n",
       "2       1  Ürün bugün elime geçti çok fazla inceleme fırs...\n",
       "3       1  Almaya karar verdim. Hemencecik geldi. Keyifle...\n",
       "4       1  Günlük kullanımınızı çok çok iyi karsılıyor kı..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('ecommercereviews.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all labels and reviews as a list\n",
    "target = dataset['Rating'].values.tolist()\n",
    "data = dataset['Review'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seperation = int(len(data) * 0.80)\n",
    "x_train, x_test = data[:seperation], data[seperation:]\n",
    "y_train, y_test = target[:seperation], target[seperation:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(243497, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get 10000 most frequently used words in our dataset\n",
    "num_words = 10000\n",
    "\n",
    "# Define tokenizer with Keras...\n",
    "# If we don't define num_words, then we use all words in our dataset.\n",
    "tokenizer = Tokenizer(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now tokenize the data\n",
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving tokenizer\n",
    "import pickle\n",
    "\n",
    "with open('turkish_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "with open('turkish_tokenizer.pickle', 'rb') as handle:\n",
    "    turkish_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenize our training data. This will only works for the words in our 10000 tokenizer. \n",
    "If a word is not in 10000 tokenized words, it will be ignored.\n",
    "\"\"\"\n",
    "x_train_tokens = turkish_tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bu fiyata bu kalite kaçırmayın derim '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 39, 5, 131, 323, 143]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tokens[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokens = turkish_tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(243497,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We will add Padding for our comments.\n",
    "In RNN, we give predefined-sized inputs. But our comments consist of different sized inputs, so we need to define\n",
    "a input size for comments. If size > comment, then add 0s for the gap, otherwise trim the comment.\n",
    "\"\"\"\n",
    "\n",
    "# How many tokens in each comment?\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "\n",
    "# Convert list to numpy array\n",
    "num_tokens = np.array(num_tokens)\n",
    "num_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.744703220162876"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In average, how many tokens in one comment?\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max token amount?\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21941"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index of max token\n",
    "np.argmax(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define max tokens for all comments\n",
    "max_tokens = np.mean(num_tokens) + 2*np.std(num_tokens) #returns float\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.97982726686571"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many tokens are smaller than max_tokens?\n",
    "np.sum(num_tokens < max_tokens) / len(num_tokens) * 100  # output: 96%. which means we will only lose info in 4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add paddings... So, all datas will be in the same size.\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194797, 59)\n",
      "(48700, 59)\n"
     ]
    }
   ],
   "source": [
    "# Check the sizes\n",
    "print(x_train_pad.shape)\n",
    "print(x_test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'çok', 2: 'bir', 3: 've', 4: 'ürün', 5: 'bu'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Keras, we can get tokens from strings but not vice versa.\n",
    "# So we will write a function to get strings from tokens\n",
    "idx = turkish_tokenizer.word_index\n",
    "\n",
    "# in idx, key value pair like 'çok': 1. But we want it reverse.\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "first_five = {k: inverse_map[k] for k in sorted(inverse_map.keys())[:5]}\n",
    "first_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 50 # vector with 50 size for every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\oztur\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now we will create an embedding layer in Keras.\n",
    "We won't use word2vec or glove, instead we create word vectors randomly.\n",
    "\"\"\"\n",
    "\n",
    "# Add embedding layer to our model.\n",
    "# embedding matris size = num_words * embedding_size -> 10.000 * 50\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='embedding_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\oztur\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# 3-layered LSTM\n",
    "model.add(LSTM(units=16, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=8, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=4, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "# Dense layer: aka fully connected layer. Consists of one neuron.\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizers import Adam\n",
    "# Adam optimizer\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 59, 50)            500000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 59, 16)            4288      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 59, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 59, 8)             800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 59, 8)             0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 4)                 208       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 505,301\n",
      "Trainable params: 505,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\oztur\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "194797/194797 [==============================] - 110s 562us/step - loss: 0.2814 - acc: 0.9429\n",
      "Epoch 2/10\n",
      "194797/194797 [==============================] - 104s 536us/step - loss: 0.2287 - acc: 0.9442\n",
      "Epoch 3/10\n",
      "194797/194797 [==============================] - 109s 562us/step - loss: 0.1899 - acc: 0.9471\n",
      "Epoch 4/10\n",
      "194797/194797 [==============================] - 106s 544us/step - loss: 0.1073 - acc: 0.9658\n",
      "Epoch 5/10\n",
      "194797/194797 [==============================] - 108s 556us/step - loss: 0.0810 - acc: 0.9752\n",
      "Epoch 6/10\n",
      "194797/194797 [==============================] - 107s 551us/step - loss: 0.0642 - acc: 0.9810\n",
      "Epoch 7/10\n",
      "194797/194797 [==============================] - 105s 540us/step - loss: 0.0524 - acc: 0.9852\n",
      "Epoch 8/10\n",
      "194797/194797 [==============================] - 115s 590us/step - loss: 0.0436 - acc: 0.9887\n",
      "Epoch 9/10\n",
      "194797/194797 [==============================] - 110s 565us/step - loss: 0.0381 - acc: 0.9906\n",
      "Epoch 10/10\n",
      "194797/194797 [==============================] - 106s 544us/step - loss: 0.0333 - acc: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20db9ec4860>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epoch -> how many times we are going to train our data.\n",
    "# batch_size -> feeding size\n",
    "model.fit(x_train_pad, y_train, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48700/48700 [==============================] - 28s 585us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20415101394971386, 0.949034907597536]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.evaluate(x_test_pad, y_test)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.3264887063655"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (result[1]) * 100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will save the model\n",
    "model.save('lstm_nlp1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"büyük bir hayal kırıklığı yaşadım bu ürün bu markaya yakışmamış\"\n",
    "text2 = \"tasarımı harika ancak kargo çok geç geldi ve ürün açılmıştı tavsiye etmem :(\"\n",
    "text3 = \"hiç resimde gösterildiği gibi değil...\"\n",
    "text4 = \"kötü yorumlar gözümü korkutmuştu ancak hiçbir sorun yaşamadım teşekkürler\"\n",
    "text5 = \"hiç bu kadar kötü bir satıcıya denk gelmemiştim. ürünü iade ediyorum\"\n",
    "text6 = \"tam bir fiyat performans ürünü\"\n",
    "text7 = \"güzel bir ürün değil\"\n",
    "texts = [text1, text2,text3,text4,text5,text6,text7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = turkish_tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[104, 2, 1032, 2333, 1466, 5, 4, 5, 1779],\n",
       " [553, 61, 82, 27, 1, 458, 33, 3, 4, 9, 1031],\n",
       " [46, 1096, 6419, 20, 50],\n",
       " [177, 735, 7728, 82, 263, 105, 326, 16],\n",
       " [46, 5, 30, 177, 2, 1717, 1244, 19, 677, 83],\n",
       " [74, 2, 28, 111, 19],\n",
       " [7, 2, 4, 50]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens\n",
    "# tokenizer1 = Tokenizer(num_words=59)\n",
    "tokens = turkish_tokenizer.texts_to_sequences(texts)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer1 = Tokenizer(num_words=10000)\n",
    "# tokenizer1.fit_on_texts(data)\n",
    "# tokens = tokenizer1.texts_to_sequences(texts)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01560246],\n",
       "       [0.47864303],\n",
       "       [0.00632283],\n",
       "       [0.9986553 ],\n",
       "       [0.00390736],\n",
       "       [0.99942845],\n",
       "       [0.932442  ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(texts):\n",
    "    tokens = turkish_tokenizer.texts_to_sequences(texts)\n",
    "    tokens_pad = pad_sequences(tokens, maxlen=max_tokens)\n",
    "    return model.predict(tokens_pad)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015602457"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c1fd3e347047d082e87c0edccd1f8e79249099522ffdbb153cc77f4302c8019"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
